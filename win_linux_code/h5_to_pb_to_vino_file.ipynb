{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = 'best_model.h5'\n",
    "\n",
    "output_graph_dir = './pb_model'\n",
    "output_graph_name = keras_model.split('.')[0] + '.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "                \n",
    "        #print('{}:{}\\n{}:{}\\n{}:{}'.format(len(freeze_var_names), freeze_var_names, \n",
    "        #                                   len(freeze_var_names), output_names, \n",
    "        #                                   len(freeze_var_names), freeze_var_names))\n",
    "        \n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 74 variables.\n",
      "INFO:tensorflow:Converted 74 variables to const ops.\n",
      "freezed pb model saved!!\n"
     ]
    }
   ],
   "source": [
    "h5_model = load_model(keras_model)\n",
    "\n",
    "frozen = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in h5_model.outputs])\n",
    "\n",
    "# import graph_def\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(frozen)\n",
    "    \n",
    "#for op in graph.get_operations():\n",
    "#    print(op.name)\n",
    "\n",
    "pb_path = graph_io.write_graph(frozen, output_graph_dir, output_graph_name, as_text=False)\n",
    "pb_path\n",
    "\n",
    "print('freezed pb model saved!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pb model to IR model using BASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/user/data/ML_SVN/mnist-OpenVINO/./pb_model/best_model.pb\n",
      "\t- Path for generated IR: \t/home/user/data/ML_SVN/mnist-OpenVINO/./best_vino_model\n",
      "\t- IR output name: \tbest_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,28,28,1]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/user/data/ML_SVN/mnist-OpenVINO/./best_vino_model/best_model.xml\n",
      "[ SUCCESS ] BIN file: /home/user/data/ML_SVN/mnist-OpenVINO/./best_vino_model/best_model.bin\n",
      "[ SUCCESS ] Total execution time: 3.29 seconds. \n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$pb_path\"\n",
    "export var1=$1\n",
    "export var2=\"./best_vino_model\"\n",
    ". /opt/intel/openvino/bin/setupvars.sh\n",
    "mo.py --input_model $var1 --output_dir $var2 --input_shape [1,28,28,1] --data_type=FP16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
